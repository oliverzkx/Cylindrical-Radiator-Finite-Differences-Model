# Task 3 ‚Äì Performance Improvement (CUDA)

## üìå Description

This task evaluates and improves the performance of a CUDA-based 2D heat propagation simulation.  
The goal is to measure and compare execution times of CPU and GPU implementations under different configurations, focusing on:

- Grid/block size selection
- Shared memory optimization
- Speedup analysis
- Numerical accuracy

## üöÄ Optimization Strategy

- **Shared memory** is used in the `row_avg_kernel` for per-row average reduction.
- **Thread block tuning**: Multiple (x, y) thread configurations are tested (e.g., 16√ó16, 32√ó8, 64√ó4, etc.).
- CPU and GPU versions run on the same input data to ensure fair comparison.
- CUDA Events are used to measure GPU timing (H2D, kernel, D2H, total).
- A mismatch checker validates GPU correctness.

## ‚öôÔ∏è Build Instructions

```bash
make
```

This compiles the following files:

- `main.cpp` ‚Äî driver and comparison logic
- `heat_utils.cpp/.h` ‚Äî CPU simulation, comparison utilities
- `heat_kernel.cu` ‚Äî CUDA kernels

## ‚ñ∂Ô∏è Usage

```
./task3 -n <rows> -m <cols> -p <steps> [-a <stop_avg>] [-c] [-t]
```

| Option | Description                                    |
| ------ | ---------------------------------------------- |
| `-n`   | Number of rows (e.g., 4096, 8192, 15360, etc.) |
| `-m`   | Number of columns                              |
| `-p`   | Number of simulation steps (e.g., 1000)        |
| `-a`   | Enable early-stop if row avg ‚â• value           |
| `-c`   | Disable CPU execution (GPU only)               |
| `-t`   | Enable timing output and speedup calculation   |

## üìä Block Size Experiments

Tested multiple block sizes to find the best performance:

| Block Size (x√óy) | GPU Time (ms) | Observations       |
| ---------------- | ------------- | ------------------ |
| 16√ó16            | ~5900         | Baseline           |
| 32√ó32            | ~7800         | Slower (too large) |
| 32√ó8             | ~4500         | ‚úÖ Good balance     |
| 64√ó4             | **~4400**     | ‚úÖ‚úÖ Best overall    |
| 128√ó2            | ~5000+        | Slower             |
| 1√ó256            | ~46000        | ‚ùå Very inefficient |

Optimal configuration used for final tests: **`dim3(64, 4)`**

## üß™ Accuracy & Speedup Results

Fixed block size to 64√ó4, tested different grid sizes (p=1000):

| Grid Size     | CPU Time (ms) | GPU Time (ms) | Speedup | Max Matrix Diff |
| ------------- | ------------- | ------------- | ------- | --------------- |
| 1024 √ó 1024   | 2038.46       | 30.65         | 66.49x  | ‚úÖ `6.5e-6`      |
| 4096 √ó 4096   | 32770.7       | 360.60        | 90.85x  | ‚úÖ `6.3e-6`      |
| 8192 √ó 8192   | 130949        | 1365.46       | 95.9x   | ‚úÖ `4.8e-5`      |
| 15360 √ó 15360 | 460586        | 4840.13       | 95.15x  | ‚úÖ `6.5e-5`      |

![](./output.png)

‚úÖ All tests passed numerical threshold of `1e-4`.
üö´ Avoid early-stopping when comparing CPU-GPU results.

## üß† Insights

- Shared memory significantly improves per-row reduction time.
- GPU performance scales almost linearly with problem size.
- Optimal thread block size for our case: **64√ó4**
- Speedups reach over **90x** compared to CPU on large grids.